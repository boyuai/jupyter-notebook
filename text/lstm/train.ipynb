{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLACEHOLDER_CORPUS_FILE_PATH = \"../../datasets/poetry.txt\"\n",
    "PLACEHOLDER_EMBEDDING_DIM = 128\n",
    "PLACEHOLDER_LSTM_DIM = 128\n",
    "PLACEHOLDER_SEQ_LENGTH = 5\n",
    "PLACEHOLDER_MODEL_PATH = \"../models/lstm_poetry.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Init\n",
    "\n",
    "import itertools\n",
    "import jieba\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed\n",
    "from keras.optimizers import SGD, Adam, Adadelta\n",
    "\n",
    "# 建立词汇表，为每种字赋予唯一的索引\n",
    "def build_vocab(text, vocab_lim):\n",
    "    word_cnt = Counter(itertools.chain(*text))\n",
    "    vocab_inv = [x[0] for x in word_cnt.most_common(vocab_lim)]\n",
    "    vocab_inv = list(sorted(vocab_inv))\n",
    "    vocab = {x: index for index, x in enumerate(vocab_inv)}\n",
    "    return vocab, vocab_inv\n",
    "\n",
    "# 处理输入文本文件\n",
    "def process_file(file_name, use_char_based_model):\n",
    "    raw_text = []\n",
    "    with open(file_name, \"r\") as f:\n",
    "        for line in f:\n",
    "            if (use_char_based_model):\n",
    "                raw_text.extend([str(ch) for ch in line])\n",
    "            else:\n",
    "                raw_text.extend([word for word in jieba.cut(line)])\n",
    "    return raw_text\n",
    "\n",
    "# 格式化文本，建立词矩阵\n",
    "def build_matrix(text, vocab, length, step):\n",
    "    M = []\n",
    "    for word in text:\n",
    "        index = vocab.get(word)\n",
    "        if index is None:\n",
    "            M.append(len(vocab))\n",
    "        else:\n",
    "            M.append(index)\n",
    "    num_sentences = len(M) // length\n",
    "    M = M[: num_sentences * length]\n",
    "    M = np.array(M)\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(0, len(M) - length, step):\n",
    "        X.append(M[i : i + length])\n",
    "        Y.append(M[i + length])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "raw_text = process_file(PLACEHOLDER_CORPUS_FILE_PATH, True)\n",
    "vocab, vocab_inv = build_vocab(raw_text, 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Config\n",
    "\n",
    "seq_length = PLACEHOLDER_SEQ_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run\n",
    "\n",
    "X, Y = build_matrix(raw_text, vocab, seq_length, 1)\n",
    "\n",
    "# 构建模型\n",
    "inputs = Input(shape=(None, ))\n",
    "embedding = Embedding(input_dim=len(vocab) + 1, output_dim=PLACEHOLDER_EMBEDDING_DIM, trainable=True)(inputs)\n",
    "lstm1 = LSTM(units=PLACEHOLDER_LSTM_DIM, return_sequences=False)(embedding)\n",
    "outputs = Dense(units=len(vocab) + 1, activation='softmax')(lstm1)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# 输出模型报告\n",
    "# model.summary()\n",
    "\n",
    "model.fit(X, Y, batch_size=512, epochs=20, verbose=1)\n",
    "model.save(PLACEHOLDER_MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
