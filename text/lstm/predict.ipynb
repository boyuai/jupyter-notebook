{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLACEHOLDER_MODEL_PATH = \"../models/lstm_poetry.h5\"\n",
    "PLACEHOLDER_CORPUS_FILE_PATH = \"../../datasets/poetry.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Init\n",
    "\n",
    "import itertools\n",
    "import jieba\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from keras.models import load_model\n",
    "\n",
    "# 建立词汇表，为每种字赋予唯一的索引\n",
    "def build_vocab(text, vocab_lim):\n",
    "    word_cnt = Counter(itertools.chain(*text))\n",
    "    vocab_inv = [x[0] for x in word_cnt.most_common(vocab_lim)]\n",
    "    vocab_inv = list(sorted(vocab_inv))\n",
    "    vocab = {x: index for index, x in enumerate(vocab_inv)}\n",
    "    return vocab, vocab_inv\n",
    "\n",
    "# 处理输入文本文件\n",
    "def process_file(file_name, use_char_based_model):\n",
    "    raw_text = []\n",
    "    with open(file_name, \"r\") as f:\n",
    "        for line in f:\n",
    "            if (use_char_based_model):\n",
    "                raw_text.extend([str(ch) for ch in line])\n",
    "            else:\n",
    "                raw_text.extend([word for word in jieba.cut(line)])\n",
    "    return raw_text\n",
    "\n",
    "# 格式化文本，建立词矩阵\n",
    "def build_matrix(text, vocab, length, step):\n",
    "    M = []\n",
    "    for word in text:\n",
    "        index = vocab.get(word)\n",
    "        if index is None:\n",
    "            M.append(len(vocab))\n",
    "        else:\n",
    "            M.append(index)\n",
    "    num_sentences = len(M) // length\n",
    "    M = M[: num_sentences * length]\n",
    "    M = np.array(M)\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(0, len(M) - length, step):\n",
    "        X.append(M[i : i + length])\n",
    "        Y.append(M[i + length])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "\n",
    "model = load_model(PLACEHOLDER_MODEL_PATH)\n",
    "raw_text = process_file(PLACEHOLDER_CORPUS_FILE_PATH, True)\n",
    "vocab, vocab_inv = build_vocab(raw_text, 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLACEHOLDER_START_TEXT = \"明月松间照\" # 限制为5个字\n",
    "PLACEHOLDER_GEN_LEN = 20\n",
    "PLACEHOLDER_TOPN = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run\n",
    "\n",
    "st = PLACEHOLDER_START_TEXT\n",
    "seq_length = 5\n",
    "\n",
    "print(st, end='')\n",
    "vocab_inv.append(' ')\n",
    "for i in range(PLACEHOLDER_GEN_LEN):\n",
    "    X_sample = np.array([[vocab.get(x, len(vocab)) for x in st]])\n",
    "    pdt = (-model.predict(X_sample))[0].argsort()[:PLACEHOLDER_TOPN]\n",
    "    if vocab_inv[pdt[0]] == '，' or vocab_inv[pdt[0]] == '。' or vocab_inv[pdt[0]] == '\\n':\n",
    "        ch = vocab_inv[pdt[0]]\n",
    "    else:\n",
    "        ch = vocab_inv[np.random.choice(pdt)]\n",
    "    print(ch, end='')\n",
    "    if len(st) == seq_length:\n",
    "        st = st[1 :] + ch\n",
    "    else:\n",
    "        st = st + ch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
