{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Init\n",
    "import urllib.request\n",
    "import io\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "from keras import backend as K\n",
    "from keras.applications.vgg16 import preprocess_input, VGG16\n",
    "from IPython.display import display\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "def postprocess_array(x):\n",
    "    t = x.copy()\n",
    "    t = t[0, :, :, :]\n",
    "    vgg_mean = [103.939, 116.779, 123.68]\n",
    "    for i in range(3):\n",
    "        t[:, :, i] += vgg_mean[i]\n",
    "    t = t[:,:,::-1]\n",
    "    t = np.clip(t, 0, 255).astype('uint8')\n",
    "    return t\n",
    "\n",
    "def load_image_online(filename):\n",
    "    with urllib.request.urlopen(filename) as url:\n",
    "        f = io.BytesIO(url.read())\n",
    "    image = Image.open(f).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "def preprocess_image(image_path, image_size = None):\n",
    "    image = load_image_online(image_path)\n",
    "    if image_size is not None:\n",
    "        if isinstance(image_size, int):\n",
    "            w, h = image.size\n",
    "            image = image.resize((image_size, image_size * h // w))\n",
    "        elif isinstance(image_size, tuple):\n",
    "            image = image.resize(image_size)\n",
    "    image = np.array(image)\n",
    "    image = np.array([image])\n",
    "    image = preprocess_input(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLACEHOLDER_CONTENT_PATH = \"http://apex.sjtu.edu.cn/public/files/avatar/20180912/lyk.jpg\"\n",
    "PLACEHOLDER_STYLE_PATH = \"https://uploads7.wikiart.org/images/giorgio-de-chirico/hector-and-andromache-1912.jpg!Large.jpg\"\n",
    "PLACEHOLDER_IMAGE_SIZE = 256\n",
    "PLACEHOLDER_CONTENT_WEIGHT = 1\n",
    "PLACEHOLDER_STYLE_WEIGHT = 1e4\n",
    "PLACEHOLDER_LEARNING_RATE = 10\n",
    "PLACEHOLDER_ITERATIONS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run\n",
    "\n",
    "content_image = preprocess_image(PLACEHOLDER_CONTENT_PATH, PLACEHOLDER_IMAGE_SIZE)\n",
    "style_image = preprocess_image(PLACEHOLDER_STYLE_PATH, (content_image.shape[2], content_image.shape[1]))\n",
    "\n",
    "content_input = K.constant(content_image)\n",
    "style_input = K.constant(style_image)\n",
    "output_image = K.variable(content_image)\n",
    "\n",
    "input_tensor = K.concatenate([content_input, style_input, output_image], axis=0)\n",
    "model = VGG16(input_tensor=input_tensor,\n",
    "              weights='../models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "              , include_top=False)\n",
    "\n",
    "output_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "\n",
    "def get_content_loss(content_features, output_features):\n",
    "    return 0.5 * K.sum(K.square(output_features - content_features))\n",
    "\n",
    "layer_feat = output_dict['block4_conv2']\n",
    "\n",
    "content_feat = layer_feat[0, :, :, :]\n",
    "\n",
    "output_feat = layer_feat[2, :, :, :]\n",
    "\n",
    "loss = PLACEHOLDER_CONTENT_WEIGHT * get_content_loss(content_feat, output_feat)\n",
    "\n",
    "def get_gram_matrix(x):\n",
    "    feature_matrix = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram_matrix = K.dot(feature_matrix, K.transpose(feature_matrix))\n",
    "    return gram_matrix\n",
    "\n",
    "def get_style_loss(style_features, output_features):\n",
    "    G = get_gram_matrix(style_features)\n",
    "    A = get_gram_matrix(output_features)\n",
    "    \n",
    "    channel_number = int(style_features.shape[2])\n",
    "    size = int(style_features.shape[0]) * int(style_features.shape[1])\n",
    "    return K.sum(K.square(G - A)) / (4.0 * (channel_number ** 2) * (size ** 2))\n",
    "\n",
    "layer_names = ['block1_conv1', 'block2_conv1',\n",
    "               'block3_conv1', 'block4_conv1',\n",
    "               'block5_conv1']\n",
    "\n",
    "for layer_name in layer_names:\n",
    "    layer_feat = output_dict[layer_name]\n",
    "    style_feat = layer_feat[1, :, :, :]\n",
    "    output_feat = layer_feat[2, :, :, :]\n",
    "    single_style_loss = get_style_loss(style_feat, output_feat)\n",
    "    loss += (PLACEHOLDER_STYLE_WEIGHT / len(layer_names)) * single_style_loss\n",
    "    \n",
    "updater = Adam(lr = PLACEHOLDER_LEARNING_RATE).get_updates(loss, [output_image])\n",
    "\n",
    "fit = K.function([], [loss, output_image], updater)\n",
    "\n",
    "out = None\n",
    "for i in range(PLACEHOLDER_ITERATIONS):\n",
    "    f_val, out = fit([])\n",
    "    img = postprocess_array(out)\n",
    "    display(array_to_img(img))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
